<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://kaluo-zz.github.io</id>
    <title>KALUO</title>
    <updated>2019-07-01T08:34:12.104Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://kaluo-zz.github.io"/>
    <link rel="self" href="https://kaluo-zz.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://kaluo-zz.github.io/images/avatar.png</logo>
    <icon>https://kaluo-zz.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, KALUO</rights>
    <entry>
        <title type="html"><![CDATA[使用TensorRT对模型进行压缩加速]]></title>
        <id>https://kaluo-zz.github.io/post/shi-yong-tensorrt-dui-mo-xing-jin-xing-ya-suo-jia-su</id>
        <link href="https://kaluo-zz.github.io/post/shi-yong-tensorrt-dui-mo-xing-jin-xing-ya-suo-jia-su">
        </link>
        <updated>2019-07-01T08:30:49.000Z</updated>
        <content type="html"><![CDATA[<h3 id="安装tensorrt5122">安装TensorRT5.1.2.2</h3>
<ol>
<li>
<p>到NVIDIA<a href="https://developer.nvidia.com/tensorrt">官网</a>下载对应Ubuntu系统的TensorRT5.1.2.2。</p>
</li>
<li>
<p>新建目录TensorRT，将下载的文件移动至该目录下，并解压：</p>
<pre><code class="language-bash">username@hostname:~/TensorRT$ tar xzvf TensorRT-5.1.2.2.Ubuntu-16.04.4.x86_64-gnu.cuda-9.0.cudnn7.5.tar.gz
</code></pre>
</li>
<li>
<p>添加TensorRT的绝对路径<code>LIB</code>目录到环境变量<code>LD_LIBRARY_PATH</code>:</p>
<pre><code class="language-bash">username@hostname:~/TensorRT/TensorRT-5.1.2.2$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/kaluo/TensorRT/TensorRT-5.1.2.25/lib
</code></pre>
</li>
<li>
<p>安装Python TensorRT wheel文件</p>
<pre><code class="language-bash">username@hostname:~/TensorRT/TensorRT-5.1.2.2$ cd python/
username@hostname:~/TensorRT/TensorRT-5.1.2.2/python$ ls
tensorrt-5.1.2.2-cp27-none-linux_x86_64.whl
tensorrt-5.1.2.2-cp34-none-linux_x86_64.whl
tensorrt-5.1.2.2-cp35-none-linux_x86_64.whl
tensorrt-5.1.2.2-cp36-none-linux_x86_64.whl
tensorrt-5.1.2.2-cp37-none-linux_x86_64.whl
username@hostname:~/TensorRT/TensorRT-5.1.2.2/python$ pip install tensorrt-5.1.2.2-cp27-none-linux_x86_64.whl #这里使用python2.7
Processing ./tensorrt-5.1.2.2-cp27-none-linux_x86_64.whl
Installing collected packages: tensorrt
Successfully installed tensorrt-5.1.2.2
</code></pre>
</li>
<li>
<p>如果需要将TensorRT和TensorFlow一起使用，则需要安装python UFF wheel文件。</p>
<pre><code class="language-bash">username@hostname:~/TensorRT/TensorRT-5.1.2.2$ cd uff/
username@hostname:~/TensorRT/TensorRT-5.1.2.2/uff$ ls
uff-0.6.3-py2.py3-none-any.whl
username@hostname:~/TensorRT/TensorRT-5.1.2.2/uff$ pip install uff-0.6.3-py2.py3-none-any.whl
</code></pre>
</li>
<li>
<p>安装python <code>graphsurgeon</code> wheel文件。</p>
<pre><code class="language-bash">username@hostname:~/TensorRT/TensorRT-5.1.2.2/graphsurgeon$ pip install graphsurgeon-0.4.0-py2.py3-none-any.whl
Processing ./graphsurgeon-0.4.0-py2.py3-none-any.whl
Installing collected packages: graphsurgeon
Successfully installed graphsurgeon-0.4.0
</code></pre>
</li>
</ol>
<blockquote>
<h3 id="43-tar-file-installation"><a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html#installing-tar">4.3. Tar File Installation</a></h3>
<p>Note: Before issuing the following commands, you'll need to replace 5.1.x.x with your specific TensorRT version. The following commands are examples.</p>
<ol>
<li>
<p>Install the following dependencies, if not already present:</p>
<ul>
<li>Install the CUDA Toolkit 9.0, 10.0 or 10.1</li>
<li>cuDNN 7.5.0</li>
<li>Python 2 or Python 3 (Optional)</li>
</ul>
</li>
<li>
<p><a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html#downloading">Download</a> the TensorRT tar file that matches the Linux distribution you are using.</p>
</li>
<li>
<p>Choose where you want to install TensorRT. This tar file will install everything into a subdirectory called TensorRT-5.1.x.x.</p>
</li>
<li>
<p>Unpack the tar file.</p>
<pre><code class="language-bash">$ tar xzvf TensorRT-5.1.x.x.Ubuntu-1x.04.x.x86_64-gnu.cuda-x.x.cudnn7.x.tar.gz
</code></pre>
<p>Where:</p>
<ul>
<li>5.1.x.x is your TensorRT version</li>
<li>Ubuntu-1x.04.x is 14.04.5, 16.04.4 or 18.04.1</li>
<li>cuda-x.x is CUDA version 9.0, 10.0, or 10.1</li>
<li>cudnn7.x is cuDNN version 7.5</li>
</ul>
<p>This directory will have sub-directories like <code>lib</code>, <code>include</code>,<code>data</code>, etc…</p>
<pre><code class="language-bash">$ ls TensorRT-5.1.x.x
bin  data  doc  graphsurgeon  include  lib  python  samples  targets  TensorRT-Release-Notes.pdf  uff
</code></pre>
</li>
<li>
<p>Add the absolute path to the TensorRT lib directory to the environment variable LD_LIBRARY_PATH:</p>
<pre><code class="language-bash">$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:&lt;eg:TensorRT-5.1.x.x/lib&gt;
</code></pre>
</li>
<li>
<p>Install the Python TensorRT wheel file.</p>
<pre><code class="language-bash">$ cd TensorRT-5.1.x.x/python
</code></pre>
<pre><code class="language-bash">$ sudo pip2 install tensorrt-5.1.x.x-cp27-none-linux_x86_64.whl
</code></pre>
<pre><code class="language-bash">$ sudo pip3 install tensorrt-5.1.x.x-cp3x-none-linux_x86_64.whl
</code></pre>
</li>
<li>
<p>Install the Python UFF wheel file. This is only required if you plan to use TensorRT with TensorFlow.</p>
<pre><code class="language-bash">$ cd TensorRT-5.1.x.x/uff
</code></pre>
<pre><code class="language-bash">$ sudo pip2 install uff-0.6.3-py2.py3-none-any.whl
</code></pre>
<pre><code class="language-bash">$ sudo pip3 install uff-0.6.3-py2.py3-none-any.whl
</code></pre>
<pre><code class="language-bash">$ which convert-to-uff
/usr/local/bin/convert-to-uff
</code></pre>
</li>
<li>
<p>Install the Python graphsurgeon wheel file.</p>
<pre><code class="language-bash">$ cd TensorRT-5.1.x.x/graphsurgeon
</code></pre>
<pre><code class="language-bash">$ sudo pip2 install graphsurgeon-0.4.0-py2.py3-none-any.whl
</code></pre>
<pre><code class="language-bash">$ sudo pip3 install graphsurgeon-0.4.0-py2.py3-none-any.whl
</code></pre>
</li>
<li>
<p>Verify the installation:</p>
<ol>
<li>Ensure that the installed files are located in the correct directories. For example, run the tree -d command to check whether all supported installed files are in place in the lib, include, data, etc… directories.</li>
<li>Build and run one of the shipped samples, for example, sampleMNIST in the installed directory. You should be able to compile and execute the sample without additional settings. For more information about sampleMNSIT, see the <a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html#mnist_sample">TensorRT Sample Support Guide</a>.</li>
<li>The Python samples are in the samples/python directory.</li>
</ol>
</li>
</ol>
</blockquote>
<h1 id="pip-install-error-with-pycuda">Pip install error with PyCUDA</h1>
<h2 id="problem">Problem</h2>
<ul>
<li>
<p>I tried to install PyCUDA using <code>pip</code>:</p>
</li>
<li>
<p>The installation tries to compile a few C++ files and it failed on the very first file with this error:</p>
<pre><code class="language-bash">In file included from src/cpp/cuda.cpp:1:0:
src/cpp/cuda.hpp:14:18: fatal error: cuda.h: No such file or directory
#include &lt;cuda.h&gt;
                ^
compilation terminated.
error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
</code></pre>
</li>
</ul>
<h2 id="investigation">Investigation</h2>
<p>This error was strange because I had set <code>CUDA_ROOT</code> and had added the <code>bin</code> path of CUDA installation to <code>PATH</code> environment variable. So, the installer should have found <code>cuda.h</code> which I could see was present in <code>$CUDA_ROOT/include</code></p>
<p>To see what was happening, I tried the same command with verbosity:</p>
<pre><code class="language-bash">$ sudo pip -vvv install pycuda
</code></pre>
<ul>
<li>Now I could see that it was failing to find <code>nvcc</code>.</li>
<li>On downloading the source code of PyCUDA and checking <code>setup.py</code>, I saw that the check for <code>nvcc</code> was used to figure out the <code>CUDA_ROOT</code> and <code>CUDA_INC_DIR</code>.</li>
<li>The reason <code>nvcc</code> was not visible was that <code>CUDA_ROOT</code> was set for my user, but this <code>PATH</code> is not visible when a command is run under <code>sudo</code>, as described here. The solution was to make the CUDA <code>bin</code> path visible to <code>sudo</code>.</li>
</ul>
<h2 id="solution">Solution</h2>
<p>To make the <code>$CUDA_ROOT/bin</code> available in <code>PATH</code> for <code>sudo</code>, we can follow the steps described here. For example, on my system with CUDA 7.0 I followed these steps:</p>
<ul>
<li>
<p>Created a new file <code>/etc/profile.d/cuda.sh</code> and added this line:</p>
<pre><code class="language-bash">export PATH=/usr/local/cuda-7.0/bin:$PATH
</code></pre>
</li>
<li>
<p>Opened <code>root</code> shell without resetting <code>PATH</code> and ran the pip installation:</p>
</li>
</ul>
<pre><code class="language-bash">$ sudo su -
$ pip install pycuda
</code></pre>
<p>This worked and PyCUDA was installed successfully!</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[pytorch获取模型信息（包括FLOPs、参数量）]]></title>
        <id>https://kaluo-zz.github.io/post/pytorch-huo-qu-mo-xing-xin-xi-bao-gua-flopscan-shu-liang</id>
        <link href="https://kaluo-zz.github.io/post/pytorch-huo-qu-mo-xing-xin-xi-bao-gua-flopscan-shu-liang">
        </link>
        <updated>2019-07-01T08:26:36.000Z</updated>
        <content type="html"><![CDATA[<p>方式一：</p>
<p>项目地址：<a href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a></p>
<p>使用方式：</p>
<ul>
<li><code>pip install torchsummary</code> 或者</li>
<li><code>git clone https://github.com/sksq96/pytorch-summary</code></li>
</ul>
<pre><code class="language-python">from torchsummary import summary
summary(your_model, input_size=(channels, H, W))
</code></pre>
<p>方式二：</p>
<p>先创建并激活一个python3的虚拟环境：</p>
<pre><code class="language-python">virtualenv python3.6-env --python=python3.6
source python3.6-env/bin/activate
</code></pre>
<p>在这个虚拟环境中使用命令：</p>
<pre><code class="language-python">pip3 install torch torchvision
pip3 install torchstat
</code></pre>
<p>进入需要统计模型信息的模型文件目录下，然后使用以下命令：</p>
<pre><code class="language-python">(python3.6-env) username@hostname:~/SSD/CSRNet-pytorch$ python
Python 3.6.7 (default, Oct 22 2018, 11:32:17)
[GCC 8.2.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; from torchstat import stat
&gt;&gt;&gt; from model import CSRNet
&gt;&gt;&gt; model = CSRNet()
&gt;&gt;&gt; stat(model,(3,248,376))
        module name  input shape output shape      params memory(MB)              MAdd             Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)
0        frontend.0    3 248 376   64 248 376      1792.0      22.77     322,265,088.0     167,100,416.0   1126144.0   23871488.0       4.15%   24997632.0
1        frontend.1   64 248 376   64 248 376         0.0      22.77       5,967,872.0       5,967,872.0  23871488.0   23871488.0       4.50%   47742976.0
2        frontend.2   64 248 376   64 248 376     36928.0      22.77   6,874,988,544.0   3,443,462,144.0  24019200.0   23871488.0       8.44%   47890688.0
3        frontend.3   64 248 376   64 248 376         0.0      22.77       5,967,872.0       5,967,872.0  23871488.0   23871488.0       0.36%   47742976.0
4        frontend.4   64 248 376   64 124 188         0.0       5.69       4,475,904.0       5,967,872.0  23871488.0    5967872.0       7.37%   29839360.0
5        frontend.5   64 124 188  128 124 188     73856.0      11.38   3,437,494,272.0   1,721,731,072.0   6263296.0   11935744.0       3.70%   18199040.0
6        frontend.6  128 124 188  128 124 188         0.0      11.38       2,983,936.0       2,983,936.0  11935744.0   11935744.0       0.15%   23871488.0
7        frontend.7  128 124 188  128 124 188    147584.0      11.38   6,874,988,544.0   3,440,478,208.0  12526080.0   11935744.0       5.02%   24461824.0
8        frontend.8  128 124 188  128 124 188         0.0      11.38       2,983,936.0       2,983,936.0  11935744.0   11935744.0       0.17%   23871488.0
9        frontend.9  128 124 188  128  62  94         0.0       2.85       2,237,952.0       2,983,936.0  11935744.0    2983936.0       2.90%   14919680.0
10      frontend.10  128  62  94  256  62  94    295168.0       5.69   3,437,494,272.0   1,720,239,104.0   4164608.0    5967872.0       2.47%   10132480.0
11      frontend.11  256  62  94  256  62  94         0.0       5.69       1,491,968.0       1,491,968.0   5967872.0    5967872.0       0.04%   11935744.0
12      frontend.12  256  62  94  256  62  94    590080.0       5.69   6,874,988,544.0   3,438,986,240.0   8328192.0    5967872.0       4.42%   14296064.0
13      frontend.13  256  62  94  256  62  94         0.0       5.69       1,491,968.0       1,491,968.0   5967872.0    5967872.0       0.04%   11935744.0
14      frontend.14  256  62  94  256  62  94    590080.0       5.69   6,874,988,544.0   3,438,986,240.0   8328192.0    5967872.0       5.04%   14296064.0
15      frontend.15  256  62  94  256  62  94         0.0       5.69       1,491,968.0       1,491,968.0   5967872.0    5967872.0       0.04%   11935744.0
16      frontend.16  256  62  94  256  31  47         0.0       1.42       1,118,976.0       1,491,968.0   5967872.0    1491968.0       1.52%    7459840.0
17      frontend.17  256  31  47  512  31  47   1180160.0       2.85   3,437,494,272.0   1,719,493,120.0   6212608.0    2983936.0       2.40%    9196544.0
18      frontend.18  512  31  47  512  31  47         0.0       2.85         745,984.0         745,984.0   2983936.0    2983936.0       0.02%    5967872.0
19      frontend.19  512  31  47  512  31  47   2359808.0       2.85   6,874,988,544.0   3,438,240,256.0  12423168.0    2983936.0       4.20%   15407104.0
20      frontend.20  512  31  47  512  31  47         0.0       2.85         745,984.0         745,984.0   2983936.0    2983936.0       0.02%    5967872.0
21      frontend.21  512  31  47  512  31  47   2359808.0       2.85   6,874,988,544.0   3,438,240,256.0  12423168.0    2983936.0       5.07%   15407104.0
22      frontend.22  512  31  47  512  31  47         0.0       2.85         745,984.0         745,984.0   2983936.0    2983936.0       0.03%    5967872.0
23        backend.0  512  31  47  512  31  47   2359808.0       2.85   6,874,988,544.0   3,438,240,256.0  12423168.0    2983936.0      10.06%   15407104.0
24        backend.1  512  31  47  512  31  47         0.0       2.85         745,984.0         745,984.0   2983936.0    2983936.0       0.02%    5967872.0
25        backend.2  512  31  47  512  31  47   2359808.0       2.85   6,874,988,544.0   3,438,240,256.0  12423168.0    2983936.0       9.12%   15407104.0
26        backend.3  512  31  47  512  31  47         0.0       2.85         745,984.0         745,984.0   2983936.0    2983936.0       0.02%    5967872.0
27        backend.4  512  31  47  512  31  47   2359808.0       2.85   6,874,988,544.0   3,438,240,256.0  12423168.0    2983936.0       8.92%   15407104.0
28        backend.5  512  31  47  512  31  47         0.0       2.85         745,984.0         745,984.0   2983936.0    2983936.0       0.03%    5967872.0
29        backend.6  512  31  47  256  31  47   1179904.0       1.42   3,437,494,272.0   1,719,120,128.0   7703552.0    1491968.0       5.79%    9195520.0
30        backend.7  256  31  47  256  31  47         0.0       1.42         372,992.0         372,992.0   1491968.0    1491968.0       0.02%    2983936.0
31        backend.8  256  31  47  128  31  47    295040.0       0.71     859,373,568.0     429,873,280.0   2672128.0     745984.0       2.61%    3418112.0
32        backend.9  128  31  47  128  31  47         0.0       0.71         186,496.0         186,496.0    745984.0     745984.0       0.01%    1491968.0
33       backend.10  128  31  47   64  31  47     73792.0       0.36     214,843,392.0     107,514,944.0   1041152.0     372992.0       1.18%    1414144.0
34       backend.11   64  31  47   64  31  47         0.0       0.36          93,248.0          93,248.0    372992.0     372992.0       0.01%     745984.0
35     output_layer   64  31  47    1  31  47        65.0       0.01         186,496.0          94,705.0    373252.0       5828.0       0.12%     379080.0
total                                          16263489.0     219.84  77,056,883,520.0  38,576,232,817.0    373252.0       5828.0     100.00%  527196872.0
==========================================================================================================================================================
Total params: 16,263,489
----------------------------------------------------------------------------------------------------------------------------------------------------------
Total memory: 219.84MB
Total MAdd: 77.06GMAdd
Total Flops: 38.58GFlops
Total MemR+W: 502.77MB
</code></pre>
<p>退出python虚拟环境</p>
<pre><code class="language-python">deactivate
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[pytorch固定网络一部分参数，训练另一部分]]></title>
        <id>https://kaluo-zz.github.io/post/pytorch-gu-ding-wang-luo-yi-bu-fen-can-shu-xun-lian-ling-yi-bu-fen</id>
        <link href="https://kaluo-zz.github.io/post/pytorch-gu-ding-wang-luo-yi-bu-fen-can-shu-xun-lian-ling-yi-bu-fen">
        </link>
        <updated>2019-07-01T08:19:17.000Z</updated>
        <content type="html"><![CDATA[<h3 id="requires_grad-false">requires_grad = False</h3>
<p>如果你想固定网络的一部分参数，训练另一部分，你可以将你想固定参数的<code>requires_grad</code>设置为<code>False</code>。例如，将VGG16卷积部分固定：</p>
<pre><code class="language-python">model = torchvision.models.vgg16(pretrained=True)
for param in model.features.parameters():
    param.requires_grad = False
</code></pre>
<h3 id="torchno_grad">torch.no_grad()</h3>
<p>上下文管理器<code>torch.no_grad()</code>是另一种实现目的的方式。在<code>torch.no_grad()</code>内，所有的操作的结果都是<code>requires_grad=False</code>，即使它们输入（inputs）的<code>requires_grad=True</code>。所以你不可能将梯度传播到<code>no_grad</code>之前的层。例如：</p>
<pre><code class="language-python">&gt;&gt;&gt; x = torch.randn(2,2)
&gt;&gt;&gt; x.requires_grad = True
&gt;&gt;&gt; lin0 = nn.Linear(2,2)
&gt;&gt;&gt; lin1 = nn.Linear(2,2)
&gt;&gt;&gt; lin2 = nn.Linear(2,2)
&gt;&gt;&gt; x1 = lin0(x)
&gt;&gt;&gt; with torch.no_grad():
...     x2 = lin1(x1) # 这里的lin1.weight.requires_grad还是为True，但是不计算梯度。
...
&gt;&gt;&gt; x3 = lin2(x2)
&gt;&gt;&gt; x3.sum().backward()
&gt;&gt;&gt; lin1.weight.requires_grad
True
&gt;&gt;&gt; print(lin0.weight.grad, lin1.weight.grad, lin2.weight.grad)
(None, None, tensor([[0.4401, 0.9459],
        [0.4401, 0.9459]]))

</code></pre>
<p>可以看到，<code>lin1.weight.requires_grad</code>为<code>True</code>，但是没有计算梯度（这是因为操作是在<code>no_grad</code>环境下完成的）。</p>
<h2 id="设置模型为eval模式">设置模型为eval模式</h2>
<p>最方便的方式是使用<code>torch.no_grad()</code>，但是你还必须将模型设置为eval模式。这可以通过调用<code>nn.Module</code>上的<code>eval()</code>方式来实现。例如：</p>
<pre><code class="language-python">with torch.no_grad():
    model = torchvision.models.vgg16(pretrained=True)
	model.eval()
</code></pre>
<p>这个操作将<code>self.training</code>属性设置为<code>False</code>，会影响类似<code>Dropout</code>和<code>BatchNorm</code>等操作的表现方式。</p>
<h2 id="获取中间节点的梯度">获取中间节点的梯度</h2>
<p>非叶子结点不会保存反向传播过程中计算得到的梯度。如果你想获得非叶子结点的梯度，需要注入hook：</p>
<pre><code class="language-python">&gt;&gt;&gt; yGrad = torch.zeros(1,1)
&gt;&gt;&gt; def extract(xVar):
...     global yGrad
...     yGrad = xVar
...
&gt;&gt;&gt; xx = Variable(torch.randn(1,1), requires_grad=True)
&gt;&gt;&gt; yy = 3*xx
&gt;&gt;&gt; zz = yy**2
&gt;&gt;&gt; yy.register_hook(extract)
&lt;torch.utils.hooks.RemovableHandle object at 0x7fa96588e9d0&gt;
&gt;&gt;&gt; print(yGrad)
tensor([[0.]])
&gt;&gt;&gt; zz.backward()
&gt;&gt;&gt; print(yGrad)
tensor([[5.1903]])
&gt;&gt;&gt;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pytorch计算图内存释放问题]]></title>
        <id>https://kaluo-zz.github.io/post/pytorch-ji-suan-tu-nei-cun-shi-fang-wen-ti</id>
        <link href="https://kaluo-zz.github.io/post/pytorch-ji-suan-tu-nei-cun-shi-fang-wen-ti">
        </link>
        <updated>2019-07-01T08:16:02.000Z</updated>
        <content type="html"><![CDATA[<p>pytoch构建的计算图是动态图，为了节约内存，所以每次一轮迭代完之后计算图就被在内存释放，所以当你想要多次<strong>backward</strong>时候就会报如下错：</p>
<pre><code class="language-python">&gt;&gt;&gt; from torch import nn
&gt;&gt;&gt; net = nn.Linear(3, 4)
&gt;&gt;&gt; input = Variable(torch.randn(2, 3), requires_grad=True)
&gt;&gt;&gt; output = net(input)
&gt;&gt;&gt; loss = torch.sum(output)
&gt;&gt;&gt; loss.backward()  # 到这计算图已经结束，计算图被释放了
&gt;&gt;&gt; loss.backward()
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/kaluo/.local/lib/python2.7/site-packages/torch/tensor.py&quot;, line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File &quot;/home/kaluo/.local/lib/python2.7/site-packages/torch/autograd/__init__.py&quot;, line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
</code></pre>
<p>之所以会报这个错，因为计算图在内存中已经被释放。但是，如果你需要多次<strong>backward</strong>只需要在第一次反向传播时候添加一个标识，如下：</p>
<pre><code class="language-python">&gt;&gt;&gt; input = Variable(torch.randn(2, 3), requires_grad=True)
&gt;&gt;&gt; output = net(input)
&gt;&gt;&gt; loss = torch.sum(output)
&gt;&gt;&gt; loss.backward(retain_graph=True) # 添加retain_graph=True标识，让计算图不被立即释放
&gt;&gt;&gt; loss.backward()

</code></pre>
<p>经过多次backward后的叶节点梯度会直接累加，所以我们一般会将其清零。</p>
<pre><code class="language-python">&gt;&gt;&gt; x = Variable(torch.FloatTensor([[1, 2]]), requires_grad=True)
&gt;&gt;&gt; y = Variable(torch.FloatTensor([[3, 4],
...                                 [5, 6]]))
&gt;&gt;&gt; loss = torch.mm(x, y)
&gt;&gt;&gt; loss.backward(torch.FloatTensor([[1, 0]]), retain_graph=True)
&gt;&gt;&gt; print(x.grad.data)
tensor([[3., 5.]])
&gt;&gt;&gt; x.grad.data.zero_() # 最后的梯度会累加到叶节点，所以叶节点清零
tensor([[0., 0.]])
&gt;&gt;&gt; loss.backward(torch.FloatTensor([[0, 1]]))
&gt;&gt;&gt; print(x.grad.data)
tensor([[4., 6.]])

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux挂载新硬盘]]></title>
        <id>https://kaluo-zz.github.io/post/linux-gua-zai-xin-ying-pan</id>
        <link href="https://kaluo-zz.github.io/post/linux-gua-zai-xin-ying-pan">
        </link>
        <updated>2019-06-26T07:39:25.000Z</updated>
        <content type="html"><![CDATA[<ol>
<li>查看新磁盘：<code>sudo fdisk -l</code>，假设要挂载的磁盘为<code>/dev/sdc</code></li>
<li>创建分区：<code>fdisk /dev/sdc</code></li>
<li>写入文件系统：<code>mkfs.ext4 /dev/sdc</code></li>
<li>新建挂载目录：<code>mkdir /mnt/data</code></li>
<li>挂载：<code>mount /dev/sdc /mnt/data</code></li>
<li>设置开机自动挂载：</li>
</ol>
<pre><code>vi /etc/fstab
# 在文件末尾添加
/dev/sdc /mnt/data ext4 defaults 0 0
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Windows系统+TP-LINK企业级无线路由器(TL-WVR1200L)+HP LaserJet P1007打印机]]></title>
        <id>https://kaluo-zz.github.io/post/windows-xi-tong-tp-link-qi-ye-ji-wu-xian-lu-you-qi-tl-wvr1200lhp-laserjet-p1007-da-yin-ji</id>
        <link href="https://kaluo-zz.github.io/post/windows-xi-tong-tp-link-qi-ye-ji-wu-xian-lu-you-qi-tl-wvr1200lhp-laserjet-p1007-da-yin-ji">
        </link>
        <updated>2019-06-25T04:22:48.000Z</updated>
        <content type="html"><![CDATA[<ol>
<li>在windows上安装HP LaserJet P1007驱动 -&gt; <a href="https://support.hp.com/cn-zh/drivers/selfservice/HP-LaserJet-P1007-Printer/3435678/model/3435679">下载</a></li>
</ol>
<p><img src="https://kaluo-zz.github.io/post-images/1561449909277.png" alt=""></p>
<ol start="2">
<li>
<p>下载安装WVR系列路由器打印服务器客户端软件 -&gt; <a href="https://service.tp-link.com.cn/detail_download_2290.html">下载</a></p>
</li>
<li>
<p>进入 <code>控制面板\硬件和声音\设备和打印机</code>，选择<code>添加打印机</code>。</p>
</li>
</ol>
<p>3.1 在弹出窗口中选择<code>我所需的打印机未列出</code></p>
<p>3.2 选择<code>通过手动设置添加本地打印机或网络打印机</code>，点击<code>下一步</code></p>
<p>3.3 按下图选择
<img src="https://kaluo-zz.github.io/post-images/1561451124496.png" alt=""></p>
<p>3.4
<img src="https://kaluo-zz.github.io/post-images/1561451151131.png" alt=""></p>
<p>3.5 选择<code>替换当前的驱动程序</code>，点击<code>下一步</code>；给打印机起名字，点击<code>下一步</code>。</p>
<p>3.6 选择<code>不共享这台打印机</code>。</p>
<ol start="4">
<li>
<p>打开打印服务器客户端软件，选中 HP LaserJet P1007，选择<code>自动连接打印机/设置自动连接打印机</code>，选择刚才创建的打印机。</p>
</li>
<li>
<p>可以打印了。</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[外网连接内网服务器]]></title>
        <id>https://kaluo-zz.github.io/post/wai-wang-lian-jie-nei-wang-fu-wu-qi</id>
        <link href="https://kaluo-zz.github.io/post/wai-wang-lian-jie-nei-wang-fu-wu-qi">
        </link>
        <updated>2019-06-23T12:34:15.000Z</updated>
        <content type="html"><![CDATA[<p><img src="https://kaluo-zz.github.io/post-images/1561344158423.jpg" alt=""></p>
<p>参考：https://www.cnblogs.com/kwongtai/p/6903420.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deformable Convolution编译问题]]></title>
        <id>https://kaluo-zz.github.io/post/deformable-convolution-bian-yi-wen-ti</id>
        <link href="https://kaluo-zz.github.io/post/deformable-convolution-bian-yi-wen-ti">
        </link>
        <updated>2019-06-23T05:32:52.000Z</updated>
        <content type="html"><![CDATA[<p>编译环境：</p>
<ol>
<li>Ubuntu 18.04</li>
<li>CUDA 9.0</li>
<li>CUDNN 7.5</li>
<li>GCC-6.4</li>
</ol>
<p>项目地址：https://github.com/CharlesShang/DCNv2</p>
<p>出错信息：</p>
<pre><code>build error:error: command '/usr/local/cuda/bin/nvcc' failed with exit status 1
</code></pre>
<p>出错原因：CUDA 9.0不支持gcc-6.4。
解决方法：替换低版本的gcc，如gcc-5。</p>
<pre><code>cd /usr/bin
sudo rm gcc
sudo rm g++
sudo ln -s /usr/bin/gcc-5 gcc
sudo ln -s /usr/bin/g++-5 g++

cd /usr/local/cuda-9.0/bin
sudo rm gcc
sudo rm g++
sudo ln -s /usr/bin/gcc-5 gcc
sudo ln -s /usr/bin/g++-5 g++
</code></pre>
<p>然后，将setup.py文件中添加<code>-DCUDA_HOST_COMPILER=/usr/bin/gcc-5</code>：</p>
<pre><code>if torch.cuda.is_available() and CUDA_HOME is not None:
        extension = CUDAExtension
        sources += source_cuda
        define_macros += [(&quot;WITH_CUDA&quot;, None)]
        extra_compile_args[&quot;nvcc&quot;] = [
            &quot;-DCUDA_HAS_FP16=1&quot;,
            &quot;-D__CUDA_NO_HALF_OPERATORS__&quot;,
            &quot;-D__CUDA_NO_HALF_CONVERSIONS__&quot;,
            &quot;-D__CUDA_NO_HALF2_OPERATORS__&quot;,
            &quot;-DCUDA_HOST_COMPILIER=/usr/bin/gcc-5&quot;,
        ]
</code></pre>
<p>删除之前编译失败的build文件：<code>rm -r build</code>;</p>
<p>重新编译：`CUDAHOSTCXX=/usr/bin/gcc-5 python3 setup.py build develop</p>
<p>注：如果是在普通用户下编译，可能出现权限问题，此时可以在编译命令后面加上<code>--user</code>。</p>
<p>参考链接：https://github.com/facebookresearch/maskrcnn-benchmark/issues/25</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[简单多边形顺逆时针判断]]></title>
        <id>https://kaluo-zz.github.io/post/jian-dan-duo-bian-xing-shun-ni-shi-zhen-pan-duan</id>
        <link href="https://kaluo-zz.github.io/post/jian-dan-duo-bian-xing-shun-ni-shi-zhen-pan-duan">
        </link>
        <updated>2019-06-15T12:18:18.000Z</updated>
        <content type="html"><![CDATA[<p>方法1：利用向量叉积求多边形面积，判断多边形面积正负。
方法2：取多边形最左边一点，判断和之前和之后点构成的向量的叉积符号。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matlab读取并解析json格式文件]]></title>
        <id>https://kaluo-zz.github.io/post/matlab-du-qu-bing-jie-xi-json-ge-shi-wen-jian</id>
        <link href="https://kaluo-zz.github.io/post/matlab-du-qu-bing-jie-xi-json-ge-shi-wen-jian">
        </link>
        <updated>2019-06-14T12:57:48.000Z</updated>
        <content type="html"><![CDATA[<pre><code>jsonData = jsondecode(fileread('file.json'));
</code></pre>
]]></content>
    </entry>
</feed>